{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===========================================\n",
    "\n",
    "\n",
    "Gebil Jibul\n",
    "\n",
    "\n",
    "Description: This program demonstrates the use of PySpark, Keras, and TensorFlow to build a deep learning algorithm that predicts the probability of heart disease based on several features from patient data.\n",
    "\n",
    "=========================================== "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification in PySpark and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"DSC 10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "sample_libsvm_data_path = 'data/sample_libsvm_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (692,[272,300,323,350,351,378,379,405,406,407,428,433,434,435,455,456,461,462,483,484,489,490,496,511,512,517,539,540,568],[-7.520689871384157e-05,-8.11577314684704e-05,3.814692771846389e-05,0.0003776490540424341,0.0003405148366194407,0.0005514455157343111,0.00040853861160969167,0.00041974673327494573,0.0008119171358670032,0.0005027708372668752,-2.392926040660149e-05,0.0005745048020902299,0.000903754642680371,7.818229700243959e-05,-2.17875519529124e-05,-3.402165821789581e-05,0.0004966517360637634,0.0008190557828370371,-8.017982139522661e-05,-2.743169403783574e-05,0.00048108322262389896,0.00048408017626778744,-8.926472920010679e-06,-0.0003414881233042728,-8.950592574121448e-05,0.0004864546911689218,-8.478698005186158e-05,-0.0004234783215831764,-7.296535777631296e-05])\n",
      "Intercept: -0.5991460286401442\n",
      "Multinomial coefficients: 2 X 692 CSRMatrix\n",
      "(0,272) 0.0001\n",
      "(0,300) 0.0001\n",
      "(0,350) -0.0002\n",
      "(0,351) -0.0001\n",
      "(0,378) -0.0003\n",
      "(0,379) -0.0002\n",
      "(0,405) -0.0002\n",
      "(0,406) -0.0004\n",
      "(0,407) -0.0002\n",
      "(0,433) -0.0003\n",
      "(0,434) -0.0005\n",
      "(0,435) -0.0001\n",
      "(0,456) 0.0\n",
      "(0,461) -0.0002\n",
      "(0,462) -0.0004\n",
      "(0,483) 0.0001\n",
      "..\n",
      "..\n",
      "Multinomial intercepts: [0.2750587585718083,-0.2750587585718083]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = spark.read.format('libsvm').load(sample_libsvm_data_path)\n",
    "\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print('Coefficients: ' + str(lrModel.coefficients))\n",
    "print('Intercept: ' + str(lrModel.intercept))\n",
    "\n",
    "# We can also use the multinomial family for binary classification\n",
    "mlr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, family='multinomial')\n",
    "\n",
    "# Fit the model\n",
    "mlrModel = mlr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
    "print('Multinomial coefficients: ' + str(mlrModel.coefficientMatrix))\n",
    "print('Multinomial intercepts: ' + str(mlrModel.interceptVector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6661906127558116\n",
      "0.6207433672479604\n",
      "0.613154125312387\n",
      "0.6059149689952394\n",
      "0.5923656241678249\n",
      "0.5898233082838019\n",
      "0.5868012627420284\n",
      "0.5844432058719141\n",
      "0.5830790068041745\n",
      "0.5807015754032354\n",
      "objectiveHistory:\n",
      "0.6833149135741672\n",
      "0.6661906127558116\n",
      "0.6207433672479604\n",
      "0.613154125312387\n",
      "0.6059149689952394\n",
      "0.5923656241678249\n",
      "0.5898233082838019\n",
      "0.5868012627420284\n",
      "0.5844432058719141\n",
      "0.5830790068041745\n",
      "0.5807015754032354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrcha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mrcha\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyspark\\sql\\context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n",
      "+---+--------------------+\n",
      "|FPR|                 TPR|\n",
      "+---+--------------------+\n",
      "|0.0|                 0.0|\n",
      "|0.0|0.017543859649122806|\n",
      "|0.0| 0.03508771929824561|\n",
      "|0.0| 0.05263157894736842|\n",
      "|0.0| 0.07017543859649122|\n",
      "|0.0| 0.08771929824561403|\n",
      "|0.0| 0.10526315789473684|\n",
      "|0.0| 0.12280701754385964|\n",
      "|0.0| 0.14035087719298245|\n",
      "|0.0| 0.15789473684210525|\n",
      "|0.0| 0.17543859649122806|\n",
      "|0.0| 0.19298245614035087|\n",
      "|0.0| 0.21052631578947367|\n",
      "|0.0| 0.22807017543859648|\n",
      "|0.0| 0.24561403508771928|\n",
      "|0.0|  0.2631578947368421|\n",
      "|0.0|  0.2807017543859649|\n",
      "|0.0|  0.2982456140350877|\n",
      "|0.0|  0.3157894736842105|\n",
      "|0.0|  0.3333333333333333|\n",
      "+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "areaUnderROC: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_f8eaac902b63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_f8eaac902b63"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the summary from the returned LogisticRegressionModel instance trained\n",
    "# in the earlier example\n",
    "trainingSummary = lrModel.summary\n",
    "\n",
    "# Obtain the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print('objectiveHistory:')\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)\n",
    "\n",
    "# Obtain the receiver-operating characteristic as a df and areaUnderROC.\n",
    "trainingSummary.roc.show()\n",
    "print('areaUnderROC: ' + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "# Set the model threshold to maximize F-Measure\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.groupBy().max('F-Measure').select('max(F-Measure)').head()\n",
    "bestThreshold = fMeasure.where(fMeasure['F-Measure'] == maxFMeasure['max(F-Measure)']) \\\n",
    "    .select('threshold').head()['threshold']\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "file_url = 'http://storage.googleapis.com/download.tensorflow.org/data/heart.csv'\n",
    "df = pd.read_csv(file_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(303, 14)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train, Val sizes: (242, 61)\n"
     ]
    }
   ],
   "source": [
    "# Creates dfs for training and validating the model\n",
    "val_df = df.sample(frac=0.2, random_state=1337)\n",
    "train_df = df.drop(val_df.index)\n",
    "\n",
    "print(f'Train, Val sizes: {len(train_df), len(val_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates TF dataset objects from dataframes\n",
    "def df_to_ds(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    labels = df.pop('target')\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(df), labels))\n",
    "    ds = ds.shuffle(buffer_size=len(df))\n",
    "    return ds\n",
    "\n",
    "# Creates TF ds objects from dfs\n",
    "train_ds = df_to_ds(train_df)\n",
    "val_ds = df_to_ds(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batches the ds - Adds dimension to shape\n",
    "train_ds = train_ds.batch(32)\n",
    "val_ds = val_ds.batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import IntegerLookup\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "\n",
    "# Normoralizes and engineers features quantitative\n",
    "def encode_numerical_feature(feature, name, ds):\n",
    "    # Create a Normalization layer for our feature\n",
    "    normalizer = Normalization()\n",
    "\n",
    "    # Prepare a dataset that only yields our feature\n",
    "    feature_ds = ds.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the statistics of the data\n",
    "    normalizer.adapt(feature_ds)\n",
    "\n",
    "    # Normalize the input feature\n",
    "    encoded_feature = normalizer(feature)\n",
    "    return encoded_feature\n",
    "\n",
    "\n",
    "# Normoralizes and engineers features qualitative\n",
    "def encode_categorical_feature(feature, name, ds, is_string):\n",
    "    lookup_class = StringLookup if is_string else IntegerLookup\n",
    "    # Create a lookup layer which will turn strings into integer indices\n",
    "    lookup = lookup_class(output_mode='binary')\n",
    "\n",
    "    # Prepare a ds that only yields our feature\n",
    "    feature_ds = ds.map(lambda x, y: x[name])\n",
    "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
    "\n",
    "    # Learn the set of possible string values and assign them a fixed integer index\n",
    "    lookup.adapt(feature_ds)\n",
    "\n",
    "    # Turn the string input into integer indices\n",
    "    encoded_feature = lookup(feature)\n",
    "    return encoded_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorizes Data by type\n",
    "data_cats = {\n",
    "    'continious': ['age', 'trestbps', 'chol', 'thalach', 'oldpeak', 'slope'],\n",
    "    'discrete': [ 'sex', 'cp', 'fbs', 'restecg', 'exang', 'ca'],\n",
    "    'string': ['thal']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_inputs = []\n",
    "all_features = []\n",
    "\n",
    "# Encodes numerically continious features\n",
    "for label in data_cats.get('continious'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label)\n",
    "    feature = encode_numerical_feature(feat_input, label, train_ds)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "# Encodes numerically discrete features\n",
    "for label in data_cats.get('discrete'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label, dtype='int64')\n",
    "    feature = encode_categorical_feature(feat_input, label, train_ds, False)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "# Encodes string features\n",
    "for label in data_cats.get('string'):\n",
    "    feat_input = keras.Input(shape=(1,), name=label, dtype='string')\n",
    "    feature = encode_categorical_feature(feat_input, label, train_ds, True)\n",
    "    \n",
    "    all_inputs.append(feat_input)\n",
    "    all_features.append(feature)\n",
    "\n",
    "all_features = layers.concatenate(all_features)\n",
    "\n",
    "x = layers.Dense(32, activation='relu')(all_features)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = keras.Model(all_inputs, output)\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 2s 87ms/step - loss: 0.7622 - accuracy: 0.4876 - val_loss: 0.6787 - val_accuracy: 0.5738\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6682 - accuracy: 0.6074 - val_loss: 0.6169 - val_accuracy: 0.6721\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6653 - accuracy: 0.5992 - val_loss: 0.5678 - val_accuracy: 0.6885\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.6066 - accuracy: 0.6860 - val_loss: 0.5284 - val_accuracy: 0.7377\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5548 - accuracy: 0.7066 - val_loss: 0.4976 - val_accuracy: 0.8033\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.5397 - accuracy: 0.7521 - val_loss: 0.4748 - val_accuracy: 0.7869\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5400 - accuracy: 0.7149 - val_loss: 0.4578 - val_accuracy: 0.7869\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.4976 - accuracy: 0.7934 - val_loss: 0.4431 - val_accuracy: 0.7869\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.4889 - accuracy: 0.7810 - val_loss: 0.4300 - val_accuracy: 0.7869\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.4735 - accuracy: 0.7727 - val_loss: 0.4195 - val_accuracy: 0.7869\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4539 - accuracy: 0.8058 - val_loss: 0.4101 - val_accuracy: 0.7869\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4283 - accuracy: 0.8058 - val_loss: 0.4022 - val_accuracy: 0.7869\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.4203 - accuracy: 0.8306 - val_loss: 0.3965 - val_accuracy: 0.8033\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4028 - accuracy: 0.8264 - val_loss: 0.3917 - val_accuracy: 0.8033\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4309 - accuracy: 0.7975 - val_loss: 0.3871 - val_accuracy: 0.8197\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3844 - accuracy: 0.8306 - val_loss: 0.3836 - val_accuracy: 0.8197\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3672 - accuracy: 0.8430 - val_loss: 0.3807 - val_accuracy: 0.8197\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3506 - accuracy: 0.8595 - val_loss: 0.3781 - val_accuracy: 0.8197\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3563 - accuracy: 0.8430 - val_loss: 0.3761 - val_accuracy: 0.8197\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3977 - accuracy: 0.8264 - val_loss: 0.3754 - val_accuracy: 0.8197\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3505 - accuracy: 0.8554 - val_loss: 0.3756 - val_accuracy: 0.8361\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3574 - accuracy: 0.8471 - val_loss: 0.3752 - val_accuracy: 0.8361\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3459 - accuracy: 0.8636 - val_loss: 0.3752 - val_accuracy: 0.8361\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3242 - accuracy: 0.8595 - val_loss: 0.3760 - val_accuracy: 0.8197\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3433 - accuracy: 0.8678 - val_loss: 0.3763 - val_accuracy: 0.8361\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3235 - accuracy: 0.8554 - val_loss: 0.3766 - val_accuracy: 0.8361\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3485 - accuracy: 0.8471 - val_loss: 0.3764 - val_accuracy: 0.8197\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3380 - accuracy: 0.8471 - val_loss: 0.3767 - val_accuracy: 0.8197\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3064 - accuracy: 0.8760 - val_loss: 0.3772 - val_accuracy: 0.8197\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3390 - accuracy: 0.8471 - val_loss: 0.3763 - val_accuracy: 0.8197\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3232 - accuracy: 0.8719 - val_loss: 0.3761 - val_accuracy: 0.8197\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3254 - accuracy: 0.8719 - val_loss: 0.3751 - val_accuracy: 0.8197\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3201 - accuracy: 0.8802 - val_loss: 0.3748 - val_accuracy: 0.8197\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3302 - accuracy: 0.8595 - val_loss: 0.3752 - val_accuracy: 0.8197\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2930 - accuracy: 0.8802 - val_loss: 0.3747 - val_accuracy: 0.8197\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.2971 - accuracy: 0.8843 - val_loss: 0.3744 - val_accuracy: 0.8197\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2761 - accuracy: 0.8884 - val_loss: 0.3751 - val_accuracy: 0.8197\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2956 - accuracy: 0.8554 - val_loss: 0.3762 - val_accuracy: 0.8197\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3074 - accuracy: 0.8636 - val_loss: 0.3770 - val_accuracy: 0.8197\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.3268 - accuracy: 0.8636 - val_loss: 0.3774 - val_accuracy: 0.8197\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2809 - accuracy: 0.8760 - val_loss: 0.3782 - val_accuracy: 0.8197\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3001 - accuracy: 0.8884 - val_loss: 0.3785 - val_accuracy: 0.8197\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2891 - accuracy: 0.8719 - val_loss: 0.3789 - val_accuracy: 0.8033\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2806 - accuracy: 0.8843 - val_loss: 0.3796 - val_accuracy: 0.8033\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2932 - accuracy: 0.8595 - val_loss: 0.3807 - val_accuracy: 0.8197\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2801 - accuracy: 0.8884 - val_loss: 0.3808 - val_accuracy: 0.8033\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2854 - accuracy: 0.8678 - val_loss: 0.3815 - val_accuracy: 0.8033\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2888 - accuracy: 0.8843 - val_loss: 0.3821 - val_accuracy: 0.8033\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2879 - accuracy: 0.8678 - val_loss: 0.3825 - val_accuracy: 0.8033\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.2589 - accuracy: 0.8719 - val_loss: 0.3833 - val_accuracy: 0.8033\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22449473430>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds, epochs=50, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This particular patient had a 23.665252327919006 percent probability of having a heart disease, as evaluated by the deep learning model.\n"
     ]
    }
   ],
   "source": [
    "sample = {\n",
    "    'age': 60,\n",
    "    'sex': 1,\n",
    "    'cp': 1,\n",
    "    'trestbps': 145,\n",
    "    'chol': 233,\n",
    "    'fbs': 1,\n",
    "    'restecg': 2,\n",
    "    'thalach': 150,\n",
    "    'exang': 0,\n",
    "    'oldpeak': 2.3,\n",
    "    'slope': 3,\n",
    "    'ca': 0,\n",
    "    'thal': 'fixed',\n",
    "}\n",
    "\n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "preds = model.predict(input_dict)\n",
    "\n",
    "print(\n",
    "    f'This particular patient had a {100 * preds[0][0]} percent probability '\n",
    "    'of having a heart disease, as evaluated by the deep learning model.'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
